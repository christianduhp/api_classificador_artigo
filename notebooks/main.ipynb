{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Após polêmica, Marine Le Pen diz que abomina n...</td>\n",
       "      <td>A candidata da direita nacionalista à Presidên...</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macron e Le Pen vão ao 2º turno na França, em ...</td>\n",
       "      <td>O centrista independente Emmanuel Macron e a d...</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apesar de larga vitória nas legislativas, Macr...</td>\n",
       "      <td>As eleições legislativas deste domingo (19) na...</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/06/189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Governo antecipa balanço, e Alckmin anuncia qu...</td>\n",
       "      <td>O número de ocorrências de homicídios dolosos ...</td>\n",
       "      <td>2015-07-24</td>\n",
       "      <td>cotidiano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/cotidiano/2015/07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Após queda em maio, a atividade econômica sobe...</td>\n",
       "      <td>A economia cresceu 0,25% no segundo trimestre,...</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2017/08/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Após polêmica, Marine Le Pen diz que abomina n...   \n",
       "1  Macron e Le Pen vão ao 2º turno na França, em ...   \n",
       "2  Apesar de larga vitória nas legislativas, Macr...   \n",
       "3  Governo antecipa balanço, e Alckmin anuncia qu...   \n",
       "4  Após queda em maio, a atividade econômica sobe...   \n",
       "\n",
       "                                                text        date   category  \\\n",
       "0  A candidata da direita nacionalista à Presidên...  2017-04-28      mundo   \n",
       "1  O centrista independente Emmanuel Macron e a d...  2017-04-23      mundo   \n",
       "2  As eleições legislativas deste domingo (19) na...  2017-06-19      mundo   \n",
       "3  O número de ocorrências de homicídios dolosos ...  2015-07-24  cotidiano   \n",
       "4  A economia cresceu 0,25% no segundo trimestre,...  2017-08-17    mercado   \n",
       "\n",
       "  subcategory                                               link  \n",
       "0         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "1         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "2         NaN  http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
       "3         NaN  http://www1.folha.uol.com.br/cotidiano/2015/07...  \n",
       "4         NaN  http://www1.folha.uol.com.br/mercado/2017/08/1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('../Data/train.csv')\n",
    "test_data = pd.read_csv('../Data/test.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_process = (title.lower() for title in train_data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queda maio atividade econômica sobe junho bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  polêmica marine le pen abomina negacionistas h...\n",
       "1  macron le pen turno frança revés siglas tradic...\n",
       "2  apesar larga vitória legislativas macron terá ...\n",
       "3  governo antecipa balanço alckmin anuncia queda...\n",
       "4       queda maio atividade econômica sobe junho bc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def process_text(doc):\n",
    "    valid_tokens = []\n",
    "    for token in doc:\n",
    "        valid = not token.is_stop and token.is_alpha\n",
    "        if valid:\n",
    "            valid_tokens.append(token.text)\n",
    "\n",
    "    if len(valid_tokens) > 2:\n",
    "        return \" \".join(valid_tokens)\n",
    "\n",
    "text_processed = [process_text(doc) for doc in nlp.pipe(text_to_process, \n",
    "                                                            batch_size = 1000,\n",
    "                                                            n_process = -1)]\n",
    "\n",
    "text_processed_df = pd.DataFrame({'title': text_processed})\n",
    "print(text_processed_df.shape)\n",
    "text_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "84466\n"
     ]
    }
   ],
   "source": [
    "text_processed_df_v2 = text_processed_df.dropna().drop_duplicates()\n",
    "print(len(text_processed_df))\n",
    "print(len(text_processed_df_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:47:43,702 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-11-12T11:47:43.702448', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-11-12 11:47:43,715 : - collecting all words and their counts\n",
      "2023-11-12 11:47:43,717 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-12 11:47:43,731 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2023-11-12 11:47:43,746 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2023-11-12 11:47:43,756 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2023-11-12 11:47:43,766 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2023-11-12 11:47:43,788 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2023-11-12 11:47:43,812 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2023-11-12 11:47:43,826 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2023-11-12 11:47:43,849 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2023-11-12 11:47:43,867 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2023-11-12 11:47:43,893 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2023-11-12 11:47:43,915 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2023-11-12 11:47:43,937 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2023-11-12 11:47:43,957 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2023-11-12 11:47:43,984 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2023-11-12 11:47:44,006 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2023-11-12 11:47:44,016 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2023-11-12 11:47:44,047 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2023-11-12 11:47:44,047 : - Creating a fresh vocabulary\n",
      "2023-11-12 11:47:44,149 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2023-11-12T11:47:44.149093', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:47:44,149 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2023-11-12T11:47:44.149093', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:47:44,289 : - deleting the raw counts dictionary of 39693 items\n",
      "2023-11-12 11:47:44,289 : - sample=0.001 downsamples 8 most-common words\n",
      "2023-11-12 11:47:44,289 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2023-11-12T11:47:44.289728', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:47:44,547 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2023-11-12 11:47:44,547 : - resetting layer weights\n",
      "2023-11-12 11:47:44,587 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-12T11:47:44.587034', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokens_list_list = [title.split(\" \") for title  in text_processed_df_v2.title]\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : - %(message)s\", level=logging.INFO)\n",
    "\n",
    "w2v_model = Word2Vec(sg = 0, \n",
    "                     window = 5,\n",
    "                     vector_size = 300,\n",
    "                     min_count = 5,\n",
    "                     alpha = 0.03,\n",
    "                     min_alpha = 0.007)\n",
    "\n",
    "\n",
    "w2v_model.build_vocab(tokens_list_list, progress_per = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:47:44,627 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-12T11:47:44.627669', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-11-12 11:47:45,612 : - EPOCH 0: training on 540242 raw words (486148 effective words) took 0.9s, 511977 effective words/s\n",
      "2023-11-12 11:47:46,627 : - EPOCH 1 - PROGRESS: at 98.14% examples, 474804 words/s, in_qsize 1, out_qsize 1\n",
      "2023-11-12 11:47:46,641 : - EPOCH 1: training on 540242 raw words (486287 effective words) took 1.0s, 481498 effective words/s\n",
      "2023-11-12 11:47:47,394 : - EPOCH 2: training on 540242 raw words (486137 effective words) took 0.7s, 651001 effective words/s\n",
      "2023-11-12 11:47:48,281 : - EPOCH 3: training on 540242 raw words (486005 effective words) took 0.9s, 557574 effective words/s\n",
      "2023-11-12 11:47:49,233 : - EPOCH 4: training on 540242 raw words (486126 effective words) took 0.9s, 525138 effective words/s\n",
      "2023-11-12 11:47:50,203 : - EPOCH 5: training on 540242 raw words (486159 effective words) took 1.0s, 509815 effective words/s\n",
      "2023-11-12 11:47:51,152 : - EPOCH 6: training on 540242 raw words (486131 effective words) took 0.9s, 522339 effective words/s\n",
      "2023-11-12 11:47:52,217 : - EPOCH 7 - PROGRESS: at 94.38% examples, 439303 words/s, in_qsize 4, out_qsize 0\n",
      "2023-11-12 11:47:52,277 : - EPOCH 7: training on 540242 raw words (486197 effective words) took 1.1s, 440923 effective words/s\n",
      "2023-11-12 11:47:53,303 : - EPOCH 8 - PROGRESS: at 87.00% examples, 419710 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:47:53,426 : - EPOCH 8: training on 540242 raw words (486137 effective words) took 1.1s, 430561 effective words/s\n",
      "2023-11-12 11:47:54,435 : - EPOCH 9: training on 540242 raw words (486090 effective words) took 1.0s, 496178 effective words/s\n",
      "2023-11-12 11:47:55,472 : - EPOCH 10 - PROGRESS: at 81.44% examples, 385527 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:47:55,702 : - EPOCH 10: training on 540242 raw words (486050 effective words) took 1.3s, 386950 effective words/s\n",
      "2023-11-12 11:47:56,724 : - EPOCH 11 - PROGRESS: at 79.60% examples, 386294 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:47:56,987 : - EPOCH 11: training on 540242 raw words (486334 effective words) took 1.3s, 383994 effective words/s\n",
      "2023-11-12 11:47:58,030 : - EPOCH 12 - PROGRESS: at 96.23% examples, 453259 words/s, in_qsize 3, out_qsize 0\n",
      "2023-11-12 11:47:58,054 : - EPOCH 12: training on 540242 raw words (486135 effective words) took 1.1s, 462208 effective words/s\n",
      "2023-11-12 11:47:59,082 : - EPOCH 13 - PROGRESS: at 90.67% examples, 437552 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:47:59,186 : - EPOCH 13: training on 540242 raw words (486157 effective words) took 1.1s, 436431 effective words/s\n",
      "2023-11-12 11:48:00,217 : - EPOCH 14 - PROGRESS: at 81.44% examples, 391066 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:48:00,407 : - EPOCH 14: training on 540242 raw words (486047 effective words) took 1.2s, 404482 effective words/s\n",
      "2023-11-12 11:48:01,447 : - EPOCH 15 - PROGRESS: at 77.75% examples, 372524 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:01,720 : - EPOCH 15: training on 540242 raw words (486001 effective words) took 1.3s, 378789 effective words/s\n",
      "2023-11-12 11:48:02,756 : - EPOCH 16 - PROGRESS: at 75.91% examples, 360627 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:03,023 : - EPOCH 16: training on 540242 raw words (486283 effective words) took 1.3s, 376973 effective words/s\n",
      "2023-11-12 11:48:04,050 : - EPOCH 17 - PROGRESS: at 98.14% examples, 475616 words/s, in_qsize 1, out_qsize 1\n",
      "2023-11-12 11:48:04,065 : - EPOCH 17: training on 540242 raw words (486170 effective words) took 1.0s, 479152 effective words/s\n",
      "2023-11-12 11:48:05,115 : - EPOCH 18 - PROGRESS: at 72.19% examples, 348319 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:05,443 : - EPOCH 18: training on 540242 raw words (486155 effective words) took 1.3s, 364148 effective words/s\n",
      "2023-11-12 11:48:06,453 : - EPOCH 19 - PROGRESS: at 90.67% examples, 435488 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:06,574 : - EPOCH 19: training on 540242 raw words (486132 effective words) took 1.1s, 430536 effective words/s\n",
      "2023-11-12 11:48:07,528 : - EPOCH 20: training on 540242 raw words (486162 effective words) took 0.9s, 529290 effective words/s\n",
      "2023-11-12 11:48:08,562 : - EPOCH 21 - PROGRESS: at 92.52% examples, 440267 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:08,615 : - EPOCH 21: training on 540242 raw words (486062 effective words) took 1.1s, 453796 effective words/s\n",
      "2023-11-12 11:48:09,640 : - EPOCH 22 - PROGRESS: at 90.67% examples, 438604 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:48:09,735 : - EPOCH 22: training on 540242 raw words (486207 effective words) took 1.1s, 441663 effective words/s\n",
      "2023-11-12 11:48:10,747 : - EPOCH 23 - PROGRESS: at 88.85% examples, 430854 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:10,855 : - EPOCH 23: training on 540242 raw words (486270 effective words) took 1.1s, 438479 effective words/s\n",
      "2023-11-12 11:48:11,844 : - EPOCH 24: training on 540242 raw words (486089 effective words) took 1.0s, 502305 effective words/s\n",
      "2023-11-12 11:48:12,808 : - EPOCH 25: training on 540242 raw words (486230 effective words) took 1.0s, 510797 effective words/s\n",
      "2023-11-12 11:48:13,836 : - EPOCH 26 - PROGRESS: at 96.23% examples, 462743 words/s, in_qsize 3, out_qsize 0\n",
      "2023-11-12 11:48:13,858 : - EPOCH 26: training on 540242 raw words (486080 effective words) took 1.0s, 473009 effective words/s\n",
      "2023-11-12 11:48:14,690 : - EPOCH 27: training on 540242 raw words (486214 effective words) took 0.8s, 596576 effective words/s\n",
      "2023-11-12 11:48:15,734 : - EPOCH 28 - PROGRESS: at 94.38% examples, 444943 words/s, in_qsize 4, out_qsize 0\n",
      "2023-11-12 11:48:15,802 : - EPOCH 28: training on 540242 raw words (486142 effective words) took 1.1s, 442542 effective words/s\n",
      "2023-11-12 11:48:16,817 : - EPOCH 29 - PROGRESS: at 94.38% examples, 458157 words/s, in_qsize 4, out_qsize 0\n",
      "2023-11-12 11:48:16,850 : - EPOCH 29: training on 540242 raw words (486193 effective words) took 1.0s, 468166 effective words/s\n",
      "2023-11-12 11:48:16,850 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584530 effective words) took 32.2s, 452603 effective words/s', 'datetime': '2023-11-12T11:48:16.850630', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584530, 16207260)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = w2v_model.corpus_count\n",
    "\n",
    "w2v_model.train(tokens_list_list, \n",
    "                total_examples = samples,\n",
    "                epochs = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iphone', 0.6156098246574402),\n",
       " ('google', 0.6147168278694153),\n",
       " ('samsung', 0.5877225995063782),\n",
       " ('amazon', 0.5694668889045715),\n",
       " ('tesla', 0.5635470747947693),\n",
       " ('microsoft', 0.5556092858314514),\n",
       " ('sony', 0.5464751124382019),\n",
       " ('renault', 0.5459845066070557),\n",
       " ('invepar', 0.5425215363502502),\n",
       " ('yahoo', 0.5413187146186829)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:48:16,956 : - storing 12924x300 projection weights into models/cbow_model.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.save_word2vec_format('models/cbow_model.txt', binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:48:24,426 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-11-12T11:48:24.426624', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-11-12 11:48:24,427 : - collecting all words and their counts\n",
      "2023-11-12 11:48:24,427 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-12 11:48:24,443 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2023-11-12 11:48:24,467 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2023-11-12 11:48:24,483 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2023-11-12 11:48:24,493 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2023-11-12 11:48:24,508 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2023-11-12 11:48:24,535 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2023-11-12 11:48:24,560 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2023-11-12 11:48:24,577 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2023-11-12 11:48:24,592 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2023-11-12 11:48:24,617 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2023-11-12 11:48:24,643 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2023-11-12 11:48:24,660 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2023-11-12 11:48:24,683 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2023-11-12 11:48:24,706 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2023-11-12 11:48:24,726 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2023-11-12 11:48:24,771 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2023-11-12 11:48:24,792 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2023-11-12 11:48:24,794 : - Creating a fresh vocabulary\n",
      "2023-11-12 11:48:24,872 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2023-11-12T11:48:24.872106', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:48:24,872 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2023-11-12T11:48:24.872106', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:48:25,042 : - deleting the raw counts dictionary of 39693 items\n",
      "2023-11-12 11:48:25,046 : - sample=0.001 downsamples 8 most-common words\n",
      "2023-11-12 11:48:25,046 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2023-11-12T11:48:25.046584', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-11-12 11:48:25,337 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2023-11-12 11:48:25,338 : - resetting layer weights\n",
      "2023-11-12 11:48:25,388 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-12T11:48:25.388178', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s : - %(message)s\", level=logging.INFO)\n",
    "\n",
    "tokens_list_list = [title.split(\" \") for title  in text_processed_df_v2.title]\n",
    "\n",
    "w2v_model_sg = Word2Vec(sg = 1, \n",
    "                     window = 5,\n",
    "                     vector_size = 300,\n",
    "                     min_count = 5,\n",
    "                     alpha = 0.03,\n",
    "                     min_alpha = 0.007)\n",
    "\n",
    "w2v_model_sg.build_vocab(tokens_list_list, progress_per = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:48:25,404 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-12T11:48:25.404730', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-11-12 11:48:26,442 : - EPOCH 0 - PROGRESS: at 31.50% examples, 150947 words/s, in_qsize 6, out_qsize 1\n",
      "2023-11-12 11:48:27,510 : - EPOCH 0 - PROGRESS: at 74.06% examples, 173325 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:28,262 : - EPOCH 0: training on 540242 raw words (486052 effective words) took 2.8s, 171902 effective words/s\n",
      "2023-11-12 11:48:29,283 : - EPOCH 1 - PROGRESS: at 40.79% examples, 196715 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:30,343 : - EPOCH 1 - PROGRESS: at 96.23% examples, 226891 words/s, in_qsize 3, out_qsize 0\n",
      "2023-11-12 11:48:30,366 : - EPOCH 1: training on 540242 raw words (486135 effective words) took 2.1s, 232635 effective words/s\n",
      "2023-11-12 11:48:31,408 : - EPOCH 2 - PROGRESS: at 48.21% examples, 228905 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:32,353 : - EPOCH 2: training on 540242 raw words (486189 effective words) took 2.0s, 247323 effective words/s\n",
      "2023-11-12 11:48:33,415 : - EPOCH 3 - PROGRESS: at 53.76% examples, 249676 words/s, in_qsize 6, out_qsize 1\n",
      "2023-11-12 11:48:34,259 : - EPOCH 3: training on 540242 raw words (486138 effective words) took 1.9s, 257664 effective words/s\n",
      "2023-11-12 11:48:35,277 : - EPOCH 4 - PROGRESS: at 50.07% examples, 241318 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:36,206 : - EPOCH 4: training on 540242 raw words (486286 effective words) took 1.9s, 251469 effective words/s\n",
      "2023-11-12 11:48:37,277 : - EPOCH 5 - PROGRESS: at 51.91% examples, 237999 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:48:38,193 : - EPOCH 5: training on 540242 raw words (486112 effective words) took 2.0s, 245657 effective words/s\n",
      "2023-11-12 11:48:39,219 : - EPOCH 6 - PROGRESS: at 50.07% examples, 242730 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:48:40,119 : - EPOCH 6: training on 540242 raw words (486216 effective words) took 1.9s, 255946 effective words/s\n",
      "2023-11-12 11:48:41,151 : - EPOCH 7 - PROGRESS: at 51.91% examples, 247481 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:41,994 : - EPOCH 7: training on 540242 raw words (486145 effective words) took 1.9s, 261368 effective words/s\n",
      "2023-11-12 11:48:43,080 : - EPOCH 8 - PROGRESS: at 51.91% examples, 235776 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:43,896 : - EPOCH 8: training on 540242 raw words (486185 effective words) took 1.9s, 258239 effective words/s\n",
      "2023-11-12 11:48:44,943 : - EPOCH 9 - PROGRESS: at 51.91% examples, 245399 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:45,854 : - EPOCH 9: training on 540242 raw words (486203 effective words) took 1.9s, 251126 effective words/s\n",
      "2023-11-12 11:48:46,875 : - EPOCH 10 - PROGRESS: at 51.91% examples, 249915 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:47,742 : - EPOCH 10: training on 540242 raw words (486167 effective words) took 1.9s, 260290 effective words/s\n",
      "2023-11-12 11:48:48,809 : - EPOCH 11 - PROGRESS: at 53.75% examples, 248044 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:49,638 : - EPOCH 11: training on 540242 raw words (486123 effective words) took 1.9s, 257296 effective words/s\n",
      "2023-11-12 11:48:50,669 : - EPOCH 12 - PROGRESS: at 51.91% examples, 249759 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:51,492 : - EPOCH 12: training on 540242 raw words (486109 effective words) took 1.8s, 265638 effective words/s\n",
      "2023-11-12 11:48:52,517 : - EPOCH 13 - PROGRESS: at 53.76% examples, 257306 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:53,356 : - EPOCH 13: training on 540242 raw words (486096 effective words) took 1.8s, 262775 effective words/s\n",
      "2023-11-12 11:48:54,382 : - EPOCH 14 - PROGRESS: at 53.76% examples, 256924 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:55,197 : - EPOCH 14: training on 540242 raw words (486122 effective words) took 1.8s, 265884 effective words/s\n",
      "2023-11-12 11:48:56,268 : - EPOCH 15 - PROGRESS: at 57.46% examples, 262643 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:56,980 : - EPOCH 15: training on 540242 raw words (486116 effective words) took 1.8s, 273788 effective words/s\n",
      "2023-11-12 11:48:58,060 : - EPOCH 16 - PROGRESS: at 53.76% examples, 244482 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:48:58,875 : - EPOCH 16: training on 540242 raw words (486281 effective words) took 1.9s, 258293 effective words/s\n",
      "2023-11-12 11:48:59,917 : - EPOCH 17 - PROGRESS: at 51.91% examples, 247769 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:00,787 : - EPOCH 17: training on 540242 raw words (486234 effective words) took 1.9s, 256534 effective words/s\n",
      "2023-11-12 11:49:01,872 : - EPOCH 18 - PROGRESS: at 51.91% examples, 237662 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:02,739 : - EPOCH 18: training on 540242 raw words (486240 effective words) took 1.9s, 252102 effective words/s\n",
      "2023-11-12 11:49:03,782 : - EPOCH 19 - PROGRESS: at 53.76% examples, 254205 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:04,611 : - EPOCH 19: training on 540242 raw words (486247 effective words) took 1.9s, 261746 effective words/s\n",
      "2023-11-12 11:49:05,701 : - EPOCH 20 - PROGRESS: at 51.91% examples, 233786 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:06,548 : - EPOCH 20: training on 540242 raw words (486173 effective words) took 1.9s, 252497 effective words/s\n",
      "2023-11-12 11:49:07,602 : - EPOCH 21 - PROGRESS: at 51.91% examples, 244865 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:08,419 : - EPOCH 21: training on 540242 raw words (486009 effective words) took 1.8s, 263456 effective words/s\n",
      "2023-11-12 11:49:09,458 : - EPOCH 22 - PROGRESS: at 48.21% examples, 229590 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:10,432 : - EPOCH 22: training on 540242 raw words (486161 effective words) took 2.0s, 243950 effective words/s\n",
      "2023-11-12 11:49:11,463 : - EPOCH 23 - PROGRESS: at 53.75% examples, 256596 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:12,267 : - EPOCH 23: training on 540242 raw words (486125 effective words) took 1.8s, 267225 effective words/s\n",
      "2023-11-12 11:49:13,288 : - EPOCH 24 - PROGRESS: at 55.60% examples, 268066 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:14,048 : - EPOCH 24: training on 540242 raw words (486122 effective words) took 1.8s, 275333 effective words/s\n",
      "2023-11-12 11:49:15,137 : - EPOCH 25 - PROGRESS: at 57.46% examples, 257727 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:15,851 : - EPOCH 25: training on 540242 raw words (486089 effective words) took 1.8s, 271095 effective words/s\n",
      "2023-11-12 11:49:16,880 : - EPOCH 26 - PROGRESS: at 48.21% examples, 232091 words/s, in_qsize 5, out_qsize 0\n",
      "2023-11-12 11:49:17,929 : - EPOCH 26 - PROGRESS: at 96.23% examples, 227438 words/s, in_qsize 3, out_qsize 0\n",
      "2023-11-12 11:49:17,964 : - EPOCH 26: training on 540242 raw words (486060 effective words) took 2.1s, 231602 effective words/s\n",
      "2023-11-12 11:49:18,993 : - EPOCH 27 - PROGRESS: at 50.07% examples, 242198 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:49:19,870 : - EPOCH 27: training on 540242 raw words (486179 effective words) took 1.9s, 258502 effective words/s\n",
      "2023-11-12 11:49:20,919 : - EPOCH 28 - PROGRESS: at 51.92% examples, 243636 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:49:21,710 : - EPOCH 28: training on 540242 raw words (486116 effective words) took 1.8s, 266242 effective words/s\n",
      "2023-11-12 11:49:22,778 : - EPOCH 29 - PROGRESS: at 57.46% examples, 262979 words/s, in_qsize 6, out_qsize 0\n",
      "2023-11-12 11:49:23,517 : - EPOCH 29: training on 540242 raw words (486208 effective words) took 1.8s, 271242 effective words/s\n",
      "2023-11-12 11:49:23,520 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584638 effective words) took 58.1s, 250973 effective words/s', 'datetime': '2023-11-12T11:49:23.520818', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584638, 16207260)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = w2v_model_sg.corpus_count\n",
    "\n",
    "w2v_model_sg.train(tokens_list_list, \n",
    "                total_examples = samples,\n",
    "                epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reguladores', 0.4149758517742157),\n",
       " ('android', 0.4046587646007538),\n",
       " ('apple', 0.3919309675693512),\n",
       " ('waze', 0.3781903088092804),\n",
       " ('toshiba', 0.36099085211753845),\n",
       " ('concorda', 0.3560527265071869),\n",
       " ('buffett', 0.35300764441490173),\n",
       " ('verizon', 0.3490554690361023),\n",
       " ('patente', 0.3490547239780426),\n",
       " ('navais', 0.34103327989578247)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_sg.wv.most_similar('google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:49:23,598 : - storing 12924x300 projection weights into models/skip_gram_model.txt\n"
     ]
    }
   ],
   "source": [
    "w2v_model_sg.wv.save_word2vec_format('models/skip_gram_model.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 11:49:29,489 : - loading projection weights from models/cbow_model.txt\n",
      "2023-11-12 11:49:35,472 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from models/cbow_model.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-11-12T11:49:35.472629', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n",
      "2023-11-12 11:49:35,485 : - loading projection weights from models/skip_gram_model.txt\n",
      "2023-11-12 11:49:40,648 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from models/skip_gram_model.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-11-12T11:49:40.648600', 'gensim': '4.3.2', 'python': '3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model_cbow = KeyedVectors.load_word2vec_format(\"models/cbow_model.txt\")\n",
    "w2v_model_sg = KeyedVectors.load_word2vec_format(\"models/skip_gram_model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"paser\", \"ner\", \"tagger\", \"textcat\"])\n",
    "\n",
    "def tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    valid_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        is_valid = not token.is_stop and token.is_alpha\n",
    "        if is_valid:\n",
    "            valid_tokens.append(token.text.lower())\n",
    "\n",
    "    return valid_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vector Combination Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vector_combination_by_sum(words, model):\n",
    "\n",
    "    result_vector = np.zeros((1, 300))\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            result_vector += model.get_vector(word)\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix of Word Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "source": [
    "def vector_matrix(texts, model):\n",
    "    rows = len(texts)\n",
    "    cols = 300\n",
    "    matrix = np.zeros((rows, cols))\n",
    "\n",
    "    for i in range(rows):\n",
    "        words = tokenizer(texts.iloc[i])\n",
    "        matrix[i] = vector_combination_by_sum(words, model)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "training_vector_matrix_cbow = vector_matrix(train_data.title, w2v_model_cbow)\n",
    "testing_vector_matrix_cbow = vector_matrix(test_data.title, w2v_model_cbow)\n",
    "\n",
    "print(training_vector_matrix_cbow.shape)\n",
    "print(testing_vector_matrix_cbow.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classifier(X_train, y_train, X_test, y_test):\n",
    "    logistic_regression = LogisticRegression(max_iter=800)\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = logistic_regression.predict(X_test)\n",
    "    classification_results = classification_report(y_test, predictions)\n",
    "    \n",
    "    print(classification_results)\n",
    "    \n",
    "    return logistic_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.80      0.71      0.76      6103\n",
      "   cotidiano       0.65      0.80      0.72      1698\n",
      "     esporte       0.93      0.87      0.90      4663\n",
      "   ilustrada       0.14      0.85      0.23       131\n",
      "     mercado       0.84      0.79      0.81      5867\n",
      "       mundo       0.75      0.84      0.79      2051\n",
      "\n",
      "    accuracy                           0.79     20513\n",
      "   macro avg       0.68      0.81      0.70     20513\n",
      "weighted avg       0.82      0.79      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rl_cbow = classifier(\n",
    "                     training_vector_matrix_cbow,\n",
    "                     train_data.category,\n",
    "                     testing_vector_matrix_cbow,\n",
    "                     test_data.category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.81      0.71      0.76      6103\n",
      "   cotidiano       0.63      0.80      0.71      1698\n",
      "     esporte       0.93      0.87      0.90      4663\n",
      "   ilustrada       0.14      0.87      0.25       131\n",
      "     mercado       0.84      0.79      0.82      5867\n",
      "       mundo       0.76      0.85      0.80      2051\n",
      "\n",
      "    accuracy                           0.80     20513\n",
      "   macro avg       0.69      0.82      0.71     20513\n",
      "weighted avg       0.82      0.80      0.81     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vector_matrix_sg = vector_matrix(train_data.title, w2v_model_sg)\n",
    "testing_vector_matrix_sg = vector_matrix(test_data.title, w2v_model_sg)\n",
    "\n",
    "rl_sg = classifier(\n",
    "                     training_vector_matrix_sg,\n",
    "                     train_data.category,\n",
    "                     testing_vector_matrix_sg,\n",
    "                     test_data.category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/rl_cbow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rl_cbow, f)\n",
    "    \n",
    "    \n",
    "with open(\"models/rl_sg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rl_sg, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
